{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d5d389-b060-412d-bdae-f13a226e3db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing matchup 0/131407\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'season' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m idx % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     69\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing matchup \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_sub)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     features = prepare_features(row[\u001b[33m\"\u001b[39m\u001b[33mSeason\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mTeam1\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mTeam2\u001b[39m\u001b[33m\"\u001b[39m], \u001b[43mseason\u001b[49m)\n\u001b[32m     72\u001b[39m     X_submit_df = pd.concat([X_submit_df, features], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Ensure columns match training data\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'season' is not defined"
     ]
    }
   ],
   "source": [
    "# Complete the training part first, this picks up where it left off\n",
    "# We'll load the saved models from the first fold and use them to make predictions\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "\n",
    "# First load the models from the folds that were completed\n",
    "def load_models(device, n_folds=5):\n",
    "    models = []\n",
    "    for fold_n in range(n_folds):\n",
    "        try:\n",
    "            model = NetModule()\n",
    "            model.to(device)\n",
    "            model.load_state_dict(torch.load(f\"netmodule_{fold_n}.pt\"))\n",
    "            model.eval()\n",
    "            models.append(model)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Model for fold {fold_n} not found, skipping\")\n",
    "            continue\n",
    "    return models\n",
    "\n",
    "\n",
    "# Load sample submission and teams data\n",
    "data_dir = f\"../datasets/march-machine-learning-mania-2025\"\n",
    "if not os.path.isdir(data_dir):\n",
    "    data_dir = f\"../input/march-machine-learning-mania-2025\"\n",
    "\n",
    "sample_sub = pd.read_csv(f\"{data_dir}/SampleSubmissionStage2.csv\")\n",
    "m_teams = pd.read_csv(f\"{data_dir}/MTeams.csv\")\n",
    "w_teams = pd.read_csv(f\"{data_dir}/WTeams.csv\")\n",
    "\n",
    "# Parse the ID column to get season and team IDs\n",
    "sample_sub[[\"Season\", \"Team1\", \"Team2\"]] = (\n",
    "    sample_sub[\"ID\"].str.split(\"_\", expand=True).astype(int)\n",
    ")\n",
    "\n",
    "\n",
    "# Function to prepare features for a matchup\n",
    "def prepare_features(season, team1, team2, season_df):\n",
    "    # Get team stats for both teams\n",
    "    team1_data = season_df[\n",
    "        (season_df[\"Season\"] == season) & (season_df[\"TeamID\"] == team1)\n",
    "    ]\n",
    "    team2_data = season_df[\n",
    "        (season_df[\"Season\"] == season) & (season_df[\"TeamID\"] == team2)\n",
    "    ]\n",
    "\n",
    "    if team1_data.empty or team2_data.empty:\n",
    "        # Fall back to average stats for the season if team not found\n",
    "        print(f\"Missing data for Season: {season}, Team1: {team1}, Team2: {team2}\")\n",
    "        team1_data = season_df[season_df[\"Season\"] == season].mean().to_frame().T\n",
    "        team2_data = season_df[season_df[\"Season\"] == season].mean().to_frame().T\n",
    "\n",
    "    # Drop the columns we won't use for prediction\n",
    "    team1_data = team1_data.drop(columns=[\"Season\", \"TeamID\"])\n",
    "    team2_data = team2_data.drop(columns=[\"Season\", \"TeamID\"])\n",
    "\n",
    "    # Rename columns to match training format\n",
    "    team1_data = team1_data.rename(columns={c: f\"{c}_1\" for c in team1_data.columns})\n",
    "    team2_data = team2_data.rename(columns={c: f\"{c}_2\" for c in team2_data.columns})\n",
    "\n",
    "    # Combine features\n",
    "    features = pd.concat([team1_data, team2_data], axis=1)\n",
    "    return features\n",
    "\n",
    "\n",
    "# Create features for all matchups in sample submission\n",
    "X_submit_df = pd.DataFrame()\n",
    "\n",
    "for idx, row in sample_sub.iterrows():\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processing matchup {idx}/{len(sample_sub)}\")\n",
    "\n",
    "    features = prepare_features(row[\"Season\"], row[\"Team1\"], row[\"Team2\"], season)\n",
    "    X_submit_df = pd.concat([X_submit_df, features], ignore_index=True)\n",
    "\n",
    "# Ensure columns match training data\n",
    "for col in X_df.columns:\n",
    "    if col not in X_submit_df.columns:\n",
    "        X_submit_df[col] = 0.0\n",
    "\n",
    "X_submit_df = X_submit_df[X_df.columns]\n",
    "\n",
    "# Scale features\n",
    "X_submit = StandardScaler().fit_transform(X_submit_df.values)\n",
    "\n",
    "# Load models and make predictions\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "models = load_models(device)\n",
    "\n",
    "# Make predictions\n",
    "all_predictions = []\n",
    "\n",
    "for model in models:\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(X_submit, dtype=torch.float32).to(device)\n",
    "        predictions = model(inputs).cpu().numpy().flatten()\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "# Average predictions across folds\n",
    "if all_predictions:\n",
    "    predictions = np.mean(all_predictions, axis=0)\n",
    "else:\n",
    "    print(\"No models found, using random predictions\")\n",
    "    predictions = np.random.normal(0, 1, size=len(X_submit))\n",
    "\n",
    "# Convert to win probabilities\n",
    "win_probs = 1 / (\n",
    "    1 + np.exp(-predictions * 0.25)\n",
    ")  # Scaling factor from the original code\n",
    "\n",
    "# Create submission file\n",
    "sample_sub[\"Pred\"] = win_probs\n",
    "sample_sub[[\"ID\", \"Pred\"]].to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# Create a binary predictions version as well (as mentioned in the original code)\n",
    "np.random.seed(42)\n",
    "binary_preds = (np.random.random(len(win_probs)) < win_probs).astype(float)\n",
    "sample_sub[\"Pred\"] = binary_preds\n",
    "sample_sub[[\"ID\", \"Pred\"]].to_csv(\"submission_binary.csv\", index=False)\n",
    "\n",
    "# Calculate Brier score for validation (if possible)\n",
    "try:\n",
    "    # We need to make predictions on the training data\n",
    "    train_predictions = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "            preds = model(inputs).cpu().numpy().flatten()\n",
    "            train_predictions.append(preds)\n",
    "\n",
    "    if train_predictions:\n",
    "        train_preds = np.mean(train_predictions, axis=0)\n",
    "        train_probs = 1 / (1 + np.exp(-train_preds * 0.25))\n",
    "\n",
    "        # The true labels: 1 if Team1 won, 0 if Team2 won\n",
    "        y_true = (train[\"Margin\"] > 0).astype(int)\n",
    "\n",
    "        brier_score = brier_score_loss(y_true, train_probs)\n",
    "        print(f\"\\nBrier score on validation data: {brier_score:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nBrier score calculation skipped - no models available\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nBrier score calculation failed: {e}\")\n",
    "\n",
    "\n",
    "# Create Top 25 Rankings for Men's and Women's teams with statistics\n",
    "def create_top_25(team_df, season_df, season_year=2025, gender=\"M\"):\n",
    "    \"\"\"Create a Top 25 ranking with statistics based on our model's features\"\"\"\n",
    "    # Get the current season data\n",
    "    current_season = season_df[season_df[\"Season\"] == season_year]\n",
    "\n",
    "    # Load team names\n",
    "    teams = m_teams if gender == \"M\" else w_teams\n",
    "    teams_in_season = pd.merge(current_season, teams, on=\"TeamID\")\n",
    "\n",
    "    # Calculate a score based on key metrics (offensive and defensive efficiency)\n",
    "    teams_in_season[\"RankingScore\"] = (\n",
    "        teams_in_season[\"Score_poss_o\"]  # Offensive points per possession\n",
    "        - teams_in_season[\"Score_poss_d\"]  # Defensive points per possession\n",
    "        + 0.5 * teams_in_season[\"FGPct_diff\"]  # Field goal percentage difference\n",
    "        + 0.5 * teams_in_season[\"AstTO_diff\"]  # Assist to turnover ratio difference\n",
    "    )\n",
    "\n",
    "    # Select most important statistics for the ranking\n",
    "    top_columns = [\n",
    "        \"TeamID\",\n",
    "        \"TeamName\",\n",
    "        \"RankingScore\",\n",
    "        \"Score_pg_o\",\n",
    "        \"Score_pg_d\",  # Points per game scored and allowed\n",
    "        \"Score_poss_o\",\n",
    "        \"Score_poss_d\",  # Points per possession scored and allowed\n",
    "        \"FGPct_o\",\n",
    "        \"FGPct_d\",  # Field goal percentage offense and defense\n",
    "        \"FGPct3_o\",\n",
    "        \"FTPct_o\",  # 3-point and free throw percentages\n",
    "        \"AstTO_o\",\n",
    "        \"AstTO_d\",  # Assist to turnover ratios\n",
    "    ]\n",
    "\n",
    "    # Create the top 25\n",
    "    top_25 = teams_in_season.sort_values(\"RankingScore\", ascending=False).head(25)[\n",
    "        top_columns\n",
    "    ]\n",
    "\n",
    "    # Rename columns for readability\n",
    "    top_25 = top_25.rename(\n",
    "        columns={\n",
    "            \"Score_pg_o\": \"PPG_Off\",\n",
    "            \"Score_pg_d\": \"PPG_Def\",\n",
    "            \"Score_poss_o\": \"PPP_Off\",\n",
    "            \"Score_poss_d\": \"PPP_Def\",\n",
    "            \"FGPct_o\": \"FG%_Off\",\n",
    "            \"FGPct_d\": \"FG%_Def\",\n",
    "            \"FGPct3_o\": \"3PT%\",\n",
    "            \"FTPct_o\": \"FT%\",\n",
    "            \"AstTO_o\": \"Ast/TO_Off\",\n",
    "            \"AstTO_d\": \"Ast/TO_Def\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Round values for better readability\n",
    "    for col in top_25.columns:\n",
    "        if col not in [\"TeamID\", \"TeamName\"]:\n",
    "            top_25[col] = top_25[col].round(3)\n",
    "\n",
    "    # Add rank column\n",
    "    top_25.insert(0, \"Rank\", range(1, 26))\n",
    "\n",
    "    return top_25\n",
    "\n",
    "\n",
    "# Create Top 25 for Men\n",
    "mens_top_25 = create_top_25(m_teams, season, 2025, \"M\")\n",
    "mens_top_25.to_csv(\"mens_top_25.csv\", index=False)\n",
    "print(\"\\nMen's Top 25:\")\n",
    "print(mens_top_25.head())\n",
    "\n",
    "# Create Top 25 for Women\n",
    "womens_top_25 = create_top_25(w_teams, season, 2025, \"W\")\n",
    "womens_top_25.to_csv(\"womens_top_25.csv\", index=False)\n",
    "print(\"\\nWomen's Top 25:\")\n",
    "print(womens_top_25.head())\n",
    "\n",
    "print(\"\\nAll outputs generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0c92c-5fe0-453e-a7dd-88a275d717fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
